<div class="help_style">
  <h1>Indexing Performance</h1>


  <p>This view helps you analyze indexing performance for currently indexed events and find bottlenecks in the splunkd data pipeline.</p>
  <h2>splunkd data pipeline</h2>
  <p>
    Data moves through the data pipeline in phases. Input data originates from inputs such as files and network feeds. As it moves through the pipeline, processors transform the data into searchable events that encapsulate knowledge.
  </p>
  <p>
    The following figure shows how input data traverses event-processing pipelines (which are the containers for processors) at index-time. Upstream from each processor is a queue for data to be processed.
  </p>
   <img height="260" width="662" src="../../static/app/sos/images/pipelines.png">
   <p>
   For more information on the splunkd data pipeline refer to:</p>
   <ul>
     <li><p><a target="_blank" class="spl-icon-external-link-xsm" href="${generateSelfHelpLink('sos.indexing_performance.distributed_conf')}">Configuration parameters and the data pipeline</a> in the <a  target="_blank" class="spl-icon-external-link-xsm" href="${generateSelfHelpLink('sos.overview.admin_manual')}">Splunk Admin manual</a></p></li>
     <li><p><a target="_blank" class="spl-icon-external-link-xsm" href="${generateSelfHelpLink('sos.indexing_performance.pipelines')}">How data moves through Splunk: the data pipeline</a> in the <a  target="_blank" class="spl-icon-external-link-xsm" href="${generateSelfHelpLink('sos.indexing_performance.deployment_manual')}">Splunk Distributed Deployment Manual</a></p></li>
   </ul>

   
  <h2>Troubleshooting bottlenecks</h2>
  
<div style="margin-bottom: 30px">
  <p>At any point in the data pipeline, data can encounter a bottleneck. Several panels in this S. o. S. view provide graphics, measuring the size of the queues entering each pipeline. You can visually see where a bottleneck occurs in the data pipeline. When this occurs, the problem usually is with one of the splunkd processors located in the data pipeline where the queue blockage occurs first (the one that is the "most downstream"). For example, if the parsing, aggregation and typing queues are all full but the indexing queue is empty, the problem lies with one of the splunkd processors in the Typing pipeline.
    </p>
</div>


  <table width="100%" border="1" cellpadding="10" cellspacing="0" bgcolor="#EFF6F6">  
  <tr valign="top">
    <td colspan="2">
    <p><strong>Estimated indexing rate</strong></p>
    <p>
      <b>Note:</b> The indexing rate in this panel is an estimate, based on data in the metrics log file (<code>group = per_index_throughput</code>). The metrics log file may not give you an exact accounting of all your indexed data. It just shows the top 10 busiest indexers for each 30 second sampling period. For example, if you have 50 active indexers, you cannot expect to see all of them accounted for in events from metrics.log.
    </p>

    <p>Select Index, Host, Source, and sourcetype to view indexed volume throughput split by that specific metadata field over the designated time span.</p>
    </td>
  </tr>

  <tr valign="top">
    <td>
    <p><strong>Queue fill ratio</strong></p>
    <p>This panel can help detect bottlenecks in the splunkd data pipeline.</p>
    <p>Choose a different statistical function from the pulldown to view how the fill percentage of event-processing queues evolves over time.</p>

    </td>
  </tr>

  <tr valign="top">
    <td colspan="2">
    <p><strong>Estimated percentage of total CPU used per splunkd processor</strong></p>
    <p>
       The estimation of CPU usage per processor is based on CPU time spent by 
       each processor, which is then divided by the elapsed time of the sample.
    </p>
    <p>
       A percentage greater than 100 indicates a CPU usage of more than one core
       for that particular processor. This is expected for 
       processors that do multi-threaded work, such as the indexer processor.
    </p>
    <p>
       As of Splunk Enterprise version 4.3, the per-processor CPU times clocked
       in metrics.log/group=pipeline are inaccurate as absolute values, resulting in
       numbers that are higher than reality. For that reason, it is important to keep
       in mind that this panel shows an <i>Estimated percentage</i> of total CPU used
       per processor.   
    </p>
    <p>
       This panel is mostly interesting when comparing the estimated CPU usage of a given
       processor against that of another one, looking for anomalies. For example,
       on a run-of-the-mill indexer you typically expect the indexer processor
       to be dominant in CPU usage over all other processors.
    </p>
    </td>
  </tr>

  <tr valign="top">
    <td colspan="2">
    <p><strong>Cumulative CPU seconds spent per indexer processor activity</strong></p>
    <p>
       This panel shows cumulative CPU time spent writing raw data against performing
       index service activities such as updating metadata, checking disk usage or
       moving buckets.
    </p>
    <p>
       If you check the "split service time by subtask" box, you will be able to see how
       much CPU time is being spent for the major index service subtasks that the indexer
       processor carries.
    </p>
    </td>
  </tr>

  </table>
  
  <h2>Searches used in this view</h2>  
  <p>
    Click the <b>View results</b> link in each panel to open the search in the Search view. You can inspect, modify, and export the search results from the Search view.
  </p>

</div>
